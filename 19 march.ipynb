{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1677e566",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application.\n",
    "answer:\n",
    "\n",
    "**Min-Max scaling**, also known as min-max normalization or feature scaling, is a data preprocessing technique used to transform the values of numeric features (variables) in a dataset into a specific range, typically between 0 and 1. The purpose of Min-Max scaling is to ensure that all features have the same scale, preventing some features from dominating others when using machine learning algorithms that are sensitive to the scale of input data.\n",
    "\n",
    "The formula to perform Min-Max scaling on a feature is as follows:\n",
    "\n",
    "\\[X_{\\text{scaled}} = \\frac{X - \\text{min}(X)}{\\text{max}(X) - \\text{min}(X)}\\]\n",
    "\n",
    "Where:\n",
    "- \\(X_{\\text{scaled}}\\) is the scaled value of the feature.\n",
    "- \\(X\\) is the original value of the feature.\n",
    "- \\(\\text{min}(X)\\) is the minimum value of the feature in the dataset.\n",
    "- \\(\\text{max}(X)\\) is the maximum value of the feature in the dataset.\n",
    "\n",
    "This transformation ensures that the minimum value in the dataset is scaled to 0, the maximum value is scaled to 1, and all other values are scaled proportionally between 0 and 1.\n",
    "\n",
    "Here's an example in Python to illustrate how Min-Max scaling is used for data preprocessing:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Sample data as a Pandas DataFrame\n",
    "data = {\n",
    "    'Age': [25, 30, 27, 22, 35],\n",
    "    'Income': [50000, 60000, 55000, 48000, 70000]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Initialize the Min-Max scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the data using the scaler\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "# Create a new DataFrame with the scaled data\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=df.columns)\n",
    "\n",
    "# Print the original and scaled DataFrames\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "print(\"\\nScaled DataFrame:\")\n",
    "print(scaled_df)\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "Original DataFrame:\n",
    "   Age  Income\n",
    "0   25   50000\n",
    "1   30   60000\n",
    "2   27   55000\n",
    "3   22   48000\n",
    "4   35   70000\n",
    "\n",
    "Scaled DataFrame:\n",
    "    Age    Income\n",
    "0  0.375  0.375000\n",
    "1  0.625  0.625000\n",
    "2  0.500  0.500000\n",
    "3  0.250  0.250000\n",
    "4  1.000  1.000000\n",
    "```\n",
    "\n",
    "In this example, we have two numeric features: 'Age' and 'Income' in our dataset. We use the `MinMaxScaler` from the `sklearn.preprocessing` module to perform Min-Max scaling on the data. The resulting scaled DataFrame ensures that all values are within the range [0, 1], preserving the relative relationships between the values within each feature. This scaled data is often used as input for machine learning algorithms to ensure that all features are on a consistent scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a40e8d1",
   "metadata": {},
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application.\n",
    "\n",
    "answer:The **Unit Vector** technique in feature scaling, also known as **Normalization**, is a data preprocessing technique used to scale the values of numeric features (variables) to have a magnitude of 1. Unlike Min-Max scaling, which scales data to a specific range (usually between 0 and 1), normalization focuses on adjusting the values while preserving their direction or relative relationships.\n",
    "\n",
    "The formula to perform Unit Vector normalization is as follows:\n",
    "\n",
    "\\[X_{\\text{normalized}} = \\frac{X}{\\|X\\|}\\]\n",
    "\n",
    "Where:\n",
    "- \\(X_{\\text{normalized}}\\) is the normalized value of the feature.\n",
    "- \\(X\\) is the original value of the feature.\n",
    "- \\(\\|X\\|\\) is the magnitude or Euclidean norm of the feature vector, calculated as \\(\\sqrt{X_1^2 + X_2^2 + \\ldots + X_n^2}\\), where \\(X_1, X_2, \\ldots, X_n\\) are the individual values in the feature.\n",
    "\n",
    "Unit Vector scaling ensures that each feature's values are transformed in such a way that they lie on the unit circle. This technique is particularly useful when the direction of the feature vector is important, such as in some machine learning algorithms like k-Nearest Neighbors (k-NN) and Principal Component Analysis (PCA).\n",
    "\n",
    "Here's an example in Python to illustrate how Unit Vector normalization is used for data preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3077973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.803773</td>\n",
       "      <td>0.551609</td>\n",
       "      <td>0.220644</td>\n",
       "      <td>0.031521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.828133</td>\n",
       "      <td>0.507020</td>\n",
       "      <td>0.236609</td>\n",
       "      <td>0.033801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.805333</td>\n",
       "      <td>0.548312</td>\n",
       "      <td>0.222752</td>\n",
       "      <td>0.034269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.800030</td>\n",
       "      <td>0.539151</td>\n",
       "      <td>0.260879</td>\n",
       "      <td>0.034784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.790965</td>\n",
       "      <td>0.569495</td>\n",
       "      <td>0.221470</td>\n",
       "      <td>0.031639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0.721557</td>\n",
       "      <td>0.323085</td>\n",
       "      <td>0.560015</td>\n",
       "      <td>0.247699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0.729654</td>\n",
       "      <td>0.289545</td>\n",
       "      <td>0.579090</td>\n",
       "      <td>0.220054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0.716539</td>\n",
       "      <td>0.330710</td>\n",
       "      <td>0.573231</td>\n",
       "      <td>0.220474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0.674671</td>\n",
       "      <td>0.369981</td>\n",
       "      <td>0.587616</td>\n",
       "      <td>0.250281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0.690259</td>\n",
       "      <td>0.350979</td>\n",
       "      <td>0.596665</td>\n",
       "      <td>0.210588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width\n",
       "0        0.803773     0.551609      0.220644     0.031521\n",
       "1        0.828133     0.507020      0.236609     0.033801\n",
       "2        0.805333     0.548312      0.222752     0.034269\n",
       "3        0.800030     0.539151      0.260879     0.034784\n",
       "4        0.790965     0.569495      0.221470     0.031639\n",
       "..            ...          ...           ...          ...\n",
       "145      0.721557     0.323085      0.560015     0.247699\n",
       "146      0.729654     0.289545      0.579090     0.220054\n",
       "147      0.716539     0.330710      0.573231     0.220474\n",
       "148      0.674671     0.369981      0.587616     0.250281\n",
       "149      0.690259     0.350979      0.596665     0.210588\n",
       "\n",
       "[150 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "df=sns.load_dataset('iris')\n",
    "df.head()\n",
    "df.columns\n",
    "a=normalize(df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']])\n",
    "pd.DataFrame(a, columns=['sepal_length', 'sepal_width', 'petal_length', 'petal_width'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5f10c8",
   "metadata": {},
   "source": [
    "\n",
    "In this example, we have two numeric features, 'X1' and 'X2', in our dataset. We manually calculate the magnitudes of the feature vectors using the Euclidean norm formula and then perform Unit Vector normalization by dividing each value in the DataFrame by its corresponding magnitude. The result is that all values are scaled so that the magnitude of each feature vector is 1, preserving their direction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee91b18c",
   "metadata": {},
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application. \n",
    "\n",
    "**Principal Component Analysis (PCA)** is a dimensionality reduction technique commonly used in data analysis and machine learning. Its primary goal is to reduce the number of features (variables) in a dataset while preserving as much of the original information as possible. PCA achieves this by transforming the original features into a new set of uncorrelated variables called principal components. These principal components are linear combinations of the original features and are ranked by the amount of variance they explain in the data.\n",
    "\n",
    "Here's how PCA works:\n",
    "\n",
    "1. **Standardization**: If your dataset contains features with different scales, it's essential to standardize them (mean centering and scaling to unit variance) before applying PCA.\n",
    "\n",
    "2. **Covariance Matrix**: PCA calculates the covariance matrix of the standardized data. The covariance matrix represents the relationships (covariances) between the features.\n",
    "\n",
    "3. **Eigenvalue Decomposition**: PCA then performs eigenvalue decomposition on the covariance matrix to obtain the eigenvalues and eigenvectors.\n",
    "\n",
    "4. **Selecting Principal Components**: The principal components are the eigenvectors of the covariance matrix. They are ordered by the amount of variance they explain, with the first principal component explaining the most variance, the second explaining the second most, and so on.\n",
    "\n",
    "5. **Dimension Reduction**: You can choose to keep a subset of the top principal components while discarding the rest. This reduces the dimensionality of the data. Typically, you select a number of principal components that retain a significant portion of the total variance (e.g., 95% of the variance).\n",
    "\n",
    "PCA is widely used in data preprocessing and feature engineering for various purposes, such as visualization, noise reduction, and improving the performance of machine learning models by reducing the risk of overfitting.\n",
    "\n",
    "Here's an example of applying PCA for dimensionality reduction in Python using scikit-learn:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the Iris dataset for demonstration\n",
    "data = load_iris()\n",
    "X = data.data  # Features\n",
    "y = data.target  # Target variable\n",
    "\n",
    "# Standardize the features (mean centering and scaling to unit variance)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=2)  # Reduce to 2 principal components\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Create a DataFrame with the reduced dimension data\n",
    "df_pca = pd.DataFrame(data=X_pca, columns=['Principal Component 1', 'Principal Component 2'])\n",
    "\n",
    "# Print the original and reduced dimension DataFrames\n",
    "print(\"Original DataFrame:\")\n",
    "print(pd.DataFrame(data=X, columns=data.feature_names).head())\n",
    "print(\"\\nReduced Dimension DataFrame:\")\n",
    "print(df_pca.head())\n",
    "```\n",
    "\n",
    "In this example, we use the Iris dataset, standardize the features, and then apply PCA to reduce the dimensionality from 4 features to 2 principal components. The resulting `df_pca` DataFrame contains the data in a reduced-dimensional space, suitable for visualization or further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac4e573",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept.\n",
    "\n",
    "**PCA (Principal Component Analysis)** can be used as a technique for **feature extraction**, which is a process of transforming the original features of a dataset into a new set of features (usually fewer in number) while retaining the most important information. PCA accomplishes feature extraction by creating new features, called principal components, that are linear combinations of the original features and capture the maximum variance in the data. These principal components can serve as a reduced and more informative representation of the data.\n",
    "\n",
    "The relationship between PCA and feature extraction can be summarized as follows:\n",
    "\n",
    "1. **Original Features**: In a dataset, you start with a set of original features (attributes) that may be high-dimensional.\n",
    "\n",
    "2. **PCA Transformation**: PCA is applied to the original feature space, creating new features (principal components) that are linear combinations of the original features.\n",
    "\n",
    "3. **Feature Selection**: You can choose to keep a subset of these principal components based on their importance (amount of variance explained). Typically, you select a reduced number of principal components that capture most of the variance in the data.\n",
    "\n",
    "4. **Feature Extraction**: The selected principal components serve as the new features, effectively reducing the dimensionality of the dataset. These extracted features are often uncorrelated with each other, making them suitable for various machine learning tasks.\n",
    "\n",
    "Here's an example of using PCA for feature extraction in Python:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the Iris dataset for demonstration\n",
    "data = load_iris()\n",
    "X = data.data  # Features\n",
    "y = data.target  # Target variable\n",
    "\n",
    "# Standardize the features (mean centering and scaling to unit variance)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA for feature extraction\n",
    "pca = PCA(n_components=2)  # Extract 2 principal components\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Create a DataFrame with the extracted features\n",
    "df_pca = pd.DataFrame(data=X_pca, columns=['Principal Component 1', 'Principal Component 2'])\n",
    "\n",
    "# Print the original and extracted feature DataFrames\n",
    "print(\"Original DataFrame:\")\n",
    "print(pd.DataFrame(data=X, columns=data.feature_names).head())\n",
    "print(\"\\nExtracted Feature DataFrame:\")\n",
    "print(df_pca.head())\n",
    "```\n",
    "\n",
    "In this example, we load the Iris dataset, standardize the features, and then use PCA to extract two principal components. The `df_pca` DataFrame contains these two extracted features, which can be used for further analysis or modeling. These extracted features are a reduced representation of the original data and are often more informative for certain tasks, such as visualization or classification.\n",
    "\n",
    "output:\n",
    "Original DataFrame:\n",
    "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
    "0                5.1               3.5                1.4               0.2\n",
    "1                4.9               3.0                1.4               0.2\n",
    "2                4.7               3.2                1.3               0.2\n",
    "3                4.6               3.1                1.5               0.2\n",
    "4                5.0               3.6                1.4               0.2\n",
    "\n",
    "Extracted Feature DataFrame:\n",
    "   Principal Component 1  Principal Component 2\n",
    "0               -2.264542                0.505704\n",
    "1               -2.086426               -0.655405\n",
    "2               -2.367950               -0.318477\n",
    "3               -2.304197               -0.575368\n",
    "4               -2.388777                0.674767\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5c0f89",
   "metadata": {},
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data.\n",
    "\n",
    "answer:\n",
    "When building a recommendation system for a food delivery service, it's essential to preprocess the data to ensure that the features are on a consistent scale and do not introduce biases into the recommendation algorithm. Min-Max scaling is one of the preprocessing techniques you can use to achieve this. Here's how you would use Min-Max scaling for the given features like price, rating, and delivery time:\n",
    "\n",
    "1. **Understand the Features**: First, you should have a good understanding of the features in your dataset. In your case, you mentioned three features: price, rating, and delivery time. \n",
    "\n",
    "2. **Data Inspection**: Examine the distribution of each feature. Check for outliers and understand the range of values for each feature. This step will help you decide whether scaling is necessary.\n",
    "\n",
    "3. **Min-Max Scaling**:\n",
    "   - **Select the Features**: Decide which features you want to scale. It's common to apply scaling to continuous or numeric features like price, rating, and delivery time.\n",
    "   \n",
    "   - **Apply Min-Max Scaling**: For each selected feature, apply Min-Max scaling separately. The scaling formula for each feature will be:\n",
    "   \n",
    "     \\[X_{\\text{scaled}} = \\frac{X - \\text{min}(X)}{\\text{max}(X) - \\text{min}(X)}\\]\n",
    "\n",
    "     - \\(X\\) represents the original values of the feature.\n",
    "     - \\(\\text{min}(X)\\) is the minimum value of that feature in your dataset.\n",
    "     - \\(\\text{max}(X)\\) is the maximum value of that feature in your dataset.\n",
    "\n",
    "   - **Normalization Range**: Decide on the range for scaling. By default, Min-Max scaling scales the feature values to the range [0, 1]. However, you can choose a different range if it's more suitable for your problem.\n",
    "\n",
    "4. **Apply the Scaled Data**: Replace the original values of the selected features in your dataset with the scaled values obtained from the Min-Max scaling process.\n",
    "\n",
    "5. **Normalization Impact**: Understand how normalization impacts the data. Min-Max scaling will ensure that all selected features are now within the chosen range (e.g., [0, 1]). This normalization helps prevent features with larger scales from dominating the recommendation process and ensures that all features have equal weight in the recommendation algorithm.\n",
    "\n",
    "6. **Recommendation Algorithm**: Use the preprocessed data as input to your recommendation algorithm (e.g., collaborative filtering, content-based filtering, or hybrid methods) to generate personalized food recommendations for users.\n",
    "\n",
    "By applying Min-Max scaling to your dataset's features, you ensure that the features are on a consistent scale, making it easier for the recommendation algorithm to provide meaningful and unbiased recommendations to users based on their preferences for price, rating, and delivery time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b2f022",
   "metadata": {},
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset.\n",
    "answeer:\n",
    "\n",
    "Using Principal Component Analysis (PCA) for dimensionality reduction in a stock price prediction project can be beneficial when dealing with a large number of features. Here's how you can use PCA to reduce the dimensionality of the dataset:\n",
    "\n",
    "1. **Data Preprocessing**:\n",
    "   - **Understand the Features**: First, gain a deep understanding of all the features in your dataset, including company financial data and market trends.\n",
    "   - **Data Cleaning**: Clean the dataset by handling missing values, outliers, and any data quality issues.\n",
    "   - **Standardization**: Standardize the features by scaling them to have a mean of 0 and a standard deviation of 1. This step is essential as PCA is sensitive to the scale of the data, and standardization ensures that all features contribute equally to the PCA process.\n",
    "\n",
    "2. **Applying PCA**:\n",
    "   - **Select Features**: Decide which features you want to include in the PCA analysis. Typically, this includes all the numeric features that are relevant to your stock price prediction task.\n",
    "   - **PCA Calculation**: Apply PCA to the selected features. PCA will create a set of new orthogonal features (principal components) that capture the most significant variance in the data.\n",
    "\n",
    "3. **Determining the Number of Components**:\n",
    "   - Calculate the explained variance ratio for each principal component. The explained variance ratio tells you how much of the total variance in the dataset is explained by each component.\n",
    "   - Plot a cumulative explained variance ratio curve. This curve helps you determine how many principal components to retain. You'll want to retain enough components to capture a high percentage (e.g., 95%) of the total variance.\n",
    "\n",
    "4. **Selecting the Number of Components**:\n",
    "   - Based on the cumulative explained variance curve, choose the number of principal components that retain the desired percentage of total variance.\n",
    "   - This step involves a trade-off between dimensionality reduction and preserving information. You should aim to retain enough components to capture essential patterns in the data while reducing dimensionality significantly.\n",
    "\n",
    "5. **Reduced-Dimension Dataset**:\n",
    "   - Transform the original dataset into a reduced-dimension dataset by keeping only the selected principal components.\n",
    "   - This new dataset will have a much lower dimensionality while still preserving a substantial portion of the variance in the original data.\n",
    "\n",
    "6. **Model Building and Evaluation**:\n",
    "   - Use the reduced-dimension dataset as input for your stock price prediction model, such as regression or time series forecasting models.\n",
    "   - Evaluate the model's performance using appropriate metrics and techniques, and iterate as needed to fine-tune the model.\n",
    "\n",
    "By applying PCA for dimensionality reduction, you achieve several benefits:\n",
    "- Reduced computational complexity, as you are working with fewer features.\n",
    "- Reduced risk of overfitting, as the model is less likely to learn noise in the data.\n",
    "- Improved interpretability, as it may be easier to understand and analyze the influence of a smaller set of principal components on stock price predictions.\n",
    "\n",
    "However, it's important to strike a balance between dimensionality reduction and retaining essential information. The choice of the number of principal components to retain should be based on your specific modeling objectives and the trade-offs involved in dimensionality reduction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3514a2a4",
   "metadata": {},
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1.\n",
    "\n",
    "answer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3454ef4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.         -0.57894737 -0.05263158  0.47368421  1.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Original dataset\n",
    "original_values = np.array([1, 5, 10, 15, 20])\n",
    "\n",
    "# Define the new_min and new_max\n",
    "new_min = -1\n",
    "new_max = 1\n",
    "\n",
    "# Calculate the minimum and maximum values in the original dataset\n",
    "min_value = np.min(original_values)\n",
    "max_value = np.max(original_values)\n",
    "\n",
    "# Apply Min-Max scaling\n",
    "scaled_values = ((original_values - min_value) / (max_value - min_value)) * (new_max - new_min) + new_min\n",
    "\n",
    "# Print the scaled values\n",
    "print(scaled_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e8f3ca",
   "metadata": {},
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n",
    "\n",
    "answer:The decision of how many principal components to retain in PCA for feature extraction depends on several factors, including the percentage of variance you want to preserve and the specific objectives of your analysis. Here are the general steps to determine how many principal components to retain:\n",
    "\n",
    "1. **Standardization**: Start by standardizing the features in your dataset. PCA is sensitive to the scale of the data, so it's essential to have all features on the same scale.\n",
    "\n",
    "2. **PCA Calculation**: Apply PCA to the standardized dataset to obtain the principal components.\n",
    "\n",
    "3. **Explained Variance Ratio**: Calculate the explained variance ratio for each principal component. The explained variance ratio tells you the proportion of the total variance in the data that is explained by each component. This can be calculated as follows:\n",
    "\n",
    "   \\[ \\text{Explained Variance Ratio} = \\frac{\\text{Variance explained by the component}}{\\text{Total variance in the data}} \\]\n",
    "\n",
    "4. **Cumulative Explained Variance**: Plot a cumulative explained variance curve. This curve shows how the cumulative explained variance increases as you add more principal components. It helps you decide how many components to retain.\n",
    "\n",
    "5. **Choose the Number of Components**: Based on your specific requirements, decide how much of the total variance you want to retain. For example, you might aim to retain 95% or 99% of the total variance. The number of components you choose should be the smallest number that achieves your desired level of explained variance.\n",
    "\n",
    "In practice, the choice of how many principal components to retain often involves a trade-off between dimensionality reduction and information preservation. Here are some considerations:\n",
    "\n",
    "- **Preserving Variance**: If you want to retain as much variance as possible to ensure that you're not losing important information, you may choose to retain a larger number of components (e.g., enough to explain 95% or 99% of the variance).\n",
    "\n",
    "- **Reducing Dimensionality**: If the goal is primarily dimensionality reduction, you might choose to retain a smaller number of components that still capture a significant portion of the variance. This can help reduce computational complexity and noise in the data.\n",
    "\n",
    "- **Interpretability**: Consider the interpretability of the extracted features. Fewer components may lead to more interpretable results.\n",
    "\n",
    "- **Computational Resources**: Keep in mind the computational resources available for your analysis. Retaining more components may require more resources.\n",
    "\n",
    "There is no one-size-fits-all answer to how many principal components to retain. It depends on the specific context of your analysis and your trade-offs between dimensionality reduction and information preservation. You can experiment with different numbers of components and evaluate the impact on your analysis to make an informed decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1897ba4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

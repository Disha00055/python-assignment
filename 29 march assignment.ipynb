{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "852aae71",
   "metadata": {},
   "source": [
    "**Q1. What is Lasso Regression, and how does it differ from other regression techniques?**\n",
    "\n",
    "Lasso Regression, or Least Absolute Shrinkage and Selection Operator, is a linear regression technique that includes a penalty term in the objective function, specifically the absolute values of the regression coefficients multiplied by a tuning parameter (lambda or alpha). The objective function for Lasso Regression is:\n",
    "\n",
    "\\[ \\text{Objective} = \\text{OLS Loss} + \\lambda \\sum_{j=1}^{p} |\\beta_j| \\]\n",
    "\n",
    "Compared to other regression techniques, Lasso has the unique property of inducing sparsity in the model, leading some coefficients to become exactly zero. This makes Lasso useful for feature selection.\n",
    "\n",
    "**Q2. What is the main advantage of using Lasso Regression in feature selection?**\n",
    "\n",
    "The main advantage of Lasso Regression in feature selection is its ability to drive some coefficients to exactly zero. This means that Lasso can automatically perform feature selection by excluding irrelevant variables from the model, providing a simpler and potentially more interpretable model.\n",
    "\n",
    "**Q3. How do you interpret the coefficients of a Lasso Regression model?**\n",
    "\n",
    "Interpreting the coefficients in a Lasso Regression model is similar to interpreting coefficients in other linear regression models. Each coefficient represents the change in the dependent variable associated with a one-unit change in the corresponding independent variable, holding other variables constant. However, the key difference is that some coefficients in Lasso may be exactly zero, indicating excluded features.\n",
    "\n",
    "**Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?**\n",
    "\n",
    "The main tuning parameter in Lasso Regression is the regularization parameter (\\( \\lambda \\) or alpha). This parameter controls the strength of the penalty applied to the absolute values of the coefficients. As \\( \\lambda \\) increases, the penalty becomes stronger, and more coefficients are likely to be driven to zero. The choice of \\( \\lambda \\) is crucial and is typically determined using cross-validation.\n",
    "\n",
    "**Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?**\n",
    "\n",
    "Lasso Regression is inherently a linear regression technique. However, it can be applied to non-linear problems by using basis functions or transforming the input features into non-linear representations before applying Lasso. Alternatively, other non-linear regression techniques may be more suitable for inherently non-linear problems.\n",
    "\n",
    "**Q6. What is the difference between Ridge Regression and Lasso Regression?**\n",
    "\n",
    "The main difference between Ridge Regression and Lasso Regression lies in the penalty term applied to the regression coefficients. Ridge uses the sum of squared coefficients, while Lasso uses the sum of absolute values of coefficients. The result is that Ridge tends to shrink coefficients towards zero but not exactly to zero, while Lasso can lead to exact zero coefficients, effectively performing feature selection.\n",
    "\n",
    "**Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?**\n",
    "\n",
    "Yes, Lasso Regression is effective in handling multicollinearity. Multicollinearity occurs when independent variables are highly correlated. Lasso tends to select one variable from a group of highly correlated variables and set the coefficients of the others to zero, thus automatically addressing multicollinearity.\n",
    "\n",
    "**Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?**\n",
    "\n",
    "The optimal value of the regularization parameter (\\( \\lambda \\)) in Lasso Regression is typically chosen through cross-validation. By evaluating the model's performance on a validation set for different values of \\( \\lambda \\), one can select the value that minimizes the prediction error. Cross-validation involves splitting the dataset into training and validation sets multiple times, fitting the model on the training set, and selecting the \\( \\lambda \\) that results in the best performance on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879cc848",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
